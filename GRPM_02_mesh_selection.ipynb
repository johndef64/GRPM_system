{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref MeSH Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook allows you to create MeSH lists to inquire the GRPM Dataset starting with a biomedical topic of choice.\n",
    " The system is based on the complete MESH datasheet.\n",
    " The system uses ChatGPT API to supply the initial biomedical terms for constructing the Ref MeSH set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    importlib.import_module('pygrpm')\n",
    "except ImportError:\n",
    "    subprocess.check_call([\"pip\", \"install\", \"git+https://github.com/johndef64/GRPM_system.git\"])\n",
    "\n",
    "from pygrpm import *\n",
    "from pygrpm.utils import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MESH.csv from 'bioportal.bioontology.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "# Get MESH.csv from https://bioportal.bioontology.org/ontologies/MESH?p=summary\n",
    "\n",
    "if not os.path.exists('ref-mesh'):\n",
    "    os.makedirs('ref-mesh')\n",
    "\n",
    "if not os.path.exists('ref-mesh/MESH.csv'):\n",
    "    \"\"\"\n",
    "    Download MESH.csv from https://bioportal.bioontology.org/ontologies/MESH\n",
    "    For the first build we employed MESH release 2022AA \n",
    "    \"\"\"\n",
    "    get_file( url='https://data.bioontology.org/ontologies/MESH/download?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb&download_format=csv', file_name='MESH.gz', dir = 'ref-mesh')\n",
    "\n",
    "    # Open the gzipped input file and the output file\n",
    "    file_path = 'ref-mesh/'\n",
    "    with gzip.open(file_path+'MESH.gz', 'rb') as input_file, open(file_path+'MESH.csv', 'wb') as output_file:\n",
    "        # Copy the content from the input gzipped file to the output file\n",
    "        shutil.copyfileobj(input_file, output_file)\n",
    "    \n",
    "    os.remove(file_path+'MESH.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pre-made datasets from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get pre-made datasets from Zenodo Repository\n",
    "#https://zenodo.org/record/8205724  DOI: 10.5281/zenodo.8205724\n",
    "dataset_releases = {'0.1':'8205724',\n",
    "                    '0.2':'14052302'}\n",
    "    \n",
    "if not os.path.exists('ref-mesh/MESH_STY_LITVAR1.csv'):\n",
    "    get_and_extract('ref-mesh', record_id=dataset_releases['0.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load full MeSH ontology release\n",
    "(skip this if MESH.csv is not provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load MESH dataframe:\n",
    "if os.path.exists('ref-mesh/MESH.csv'):\n",
    "    df = pd.read_csv('ref-mesh/MESH.csv', low_memory=False)\n",
    "else:\n",
    "    print(\"no MESH.csv available.\\n => skip to Section 2\")\n",
    "\n",
    "#info on Proprieties: https://bioportal.bioontology.org/ontologies/MESH?p=properties\n",
    "#Mesh Browser: https://meshb.nlm.nih.gov/\n",
    "\n",
    "# AQL: Allowable Qualifiers\n",
    "AQL = [\"blood (BL)\",\"cerebrospinal fluid (CF)\",\"chemically induced (CI)\",\"classification (CL)\",\"complications (CO)\",\"congenital (CN)\",\"diagnosis (DI)\",\"diagnostic imaging (DG)\",\"diet therapy (DH)\",\"drug therapy (DT)\",\"economics (EC)\",\"embryology (EM)\",\"enzymology (EN)\",\"epidemiology (EP)\",\"ethnology (EH)\",\"etiology (ET)\",\"genetics (GE)\",\"history (HI)\",\"immunology (IM)\",\"metabolism (ME)\",\"microbiology (MI)\",\"mortality (MO)\",\"nursing (NU)\",\"parasitology (PS)\",\"pathology (PA)\",\"physiopathology (PP)\",\"prevention & control (PC)\",\"psychology (PX)\",\"radiotherapy (RT)\",\"rehabilitation (RH)\",\"surgery (SU)\",\"therapy (TH)\",\"urine (UR)\",\"veterinary (VE)\",\"virology (VI)\"]\n",
    "aql=pd.DataFrame(AQL)\n",
    "aql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('display MeSH data:\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many MeSH has mapping qualifier (subheadings)?\n",
    "all_mesh = df['Preferred Label'].drop_duplicates()\n",
    "mapping_qf = df[['Mapping qualifier of','Preferred Label']].drop_duplicates().dropna()\n",
    "has = df[['Has mapping qualifier','Preferred Label']].drop_duplicates().dropna()\n",
    "has_len = has['Preferred Label'].nunique()\n",
    "\n",
    "print('Total number of MeSH terms:\\n', df['Preferred Label'].nunique())\n",
    "print('\\nHow many Mesh has mapping qualifier (subheadings)?\\n',has_len)\n",
    "print(' %', round((has_len/len(all_mesh)*100),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search for single exact value\n",
    "mesh = 'Disease or Syndrome'\n",
    "mesh = 'Heart Failure'\n",
    "print('search for single exact value:')\n",
    "df[df['Preferred Label']==mesh].T.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract subset of Mesh\n",
    "str_1 = 'Fabry'\n",
    "str_2 = 'Lysosomal Sto'\n",
    "str_3 = ''\n",
    "str_full = str_2 or str_1 or str_3\n",
    "\n",
    "ref_search = df[df['Preferred Label'].str.contains(str_full).fillna(False)]\n",
    "print('look for mesh containing \"',str_1,',',str_2,',',str_3,  '\":')\n",
    "ref_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract subset of all semantic types\n",
    "df_sem = df[df['Class ID'].str.contains('STY').fillna(False)]\n",
    "df_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characterize a Semantic Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many mesh for semantic type?\n",
    "semantictype = 'T192'\n",
    "sam_label = df[df['Class ID'].str.contains(semantictype).fillna(False)].reset_index().at[0,'Preferred Label']\n",
    "# Get Semantic Type name for STY ID\n",
    "print('semantic type:', sam_label)\n",
    "\n",
    "mask2 = df['Semantic Types'].str.contains(semantictype).fillna(False)\n",
    "mesh_sem = df[mask2][['Class ID','Preferred Label','Definitions','Semantic Types']]\n",
    "print('number of mesh:', len(mesh_sem))\n",
    "print('\\nMesh for \"',semantictype,sam_label,'\" semantic type:')\n",
    "\n",
    "mesh_sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get single mesh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search for single Mesh\n",
    "mesh_classid ='D004048'\n",
    "maskmesh = df['Class ID'].str.contains(mesh_classid).fillna(False)\n",
    "\n",
    "sty_classid = df[maskmesh]['Semantic Types'].reset_index().iat[0, 1][-4:]\n",
    "\n",
    "masksty = df['Class ID'].str.contains(sty_classid).fillna(False)\n",
    "\n",
    "masked = df[maskmesh][['Semantic Types','Preferred Label','Definitions']].reset_index()\n",
    "\n",
    "#get mesh definition:\n",
    "print('mesh:', masked.at[0,'Preferred Label'],\n",
    "      '\\nsemantic type:',df[masksty].reset_index().at[0,'Preferred Label'],sty_classid,\n",
    "      '\\ndescrition:',masked.at[0,'Definitions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Row aggregation\n",
    "data_mesh = []\n",
    "data_sty = ['T021','T116']\n",
    "dfd = pd.DataFrame()\n",
    "for i in data_sty:\n",
    "    mask = df['Semantic Types'].str.contains(i).fillna(False) # aggregate mesh for sty\n",
    "    dfd = pd.concat([dfd, df[mask]])\n",
    "dfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load/create GRPM-MeSH dataset (required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:47:05.796565300Z",
     "start_time": "2025-02-21T10:46:24.298113700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESH_STY created\n"
     ]
    }
   ],
   "source": [
    "# Subset MeSH dataset\n",
    "\n",
    "#set options:\n",
    "import_mesh_sty = True\n",
    "\n",
    "# concat the exploded mesh table or import prearranged csv\n",
    "if os.path.isfile('ref-mesh/MESH_STY.csv') and import_mesh_sty:\n",
    "    mesh_large_df_sty = pd.read_csv('ref-mesh/MESH_STY.csv', index_col=0)\n",
    "    print('MESH_STY sty imported from csv')\n",
    "elif import_mesh_sty:\n",
    "        if os.path.exists('ref-mesh/MESH.csv'):\n",
    "            df = pd.read_csv('ref-mesh/MESH.csv', low_memory=False)\n",
    "            # correct list format\n",
    "            df['STY_ID'] = df['Semantic Types'].str.replace(r'http://purl.bioontology.org/ontology/STY/','', regex = False)\n",
    "            start_word = '[\\\"'\n",
    "            end_word = '\\\"]'\n",
    "            df['STY_ID'] = f'{start_word} ' + df['STY_ID'] + f' {end_word}'\n",
    "            df['STY_ID'] = df['STY_ID'].str.replace(' ','', regex = False)\n",
    "            df['STY_ID'] = df['STY_ID'].str.replace('|','\\\",\\\"', regex = False)\n",
    "            #print(df['STY_ID'].isna().sum())\n",
    "            df.dropna(subset=['STY_ID'], inplace=True)\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "            from ast import literal_eval\n",
    "            df['STY_ID'] = df['STY_ID'].apply(literal_eval)\n",
    "\n",
    "            # Import exhcange table sty-code\n",
    "            sty = pd.read_csv('ref-mesh/MeshSTY-code.csv',sep=';')\n",
    "            sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "            sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "            #print(sty['Semantic Types'].nunique())\n",
    "\n",
    "            #mesh_large = pd.DataFrame()\n",
    "            mesh_large = []\n",
    "            for i in range(len(df)):\n",
    "                for sem in df['STY_ID'][i]: #dfrspost = mother table\n",
    "                    out = df['Preferred Label'][i],df['Class ID'][i],sem,df['Synonyms'][i],df['Parents'][i],df['CUI'][i],df['AQL'][i],df['TERMUI'][i]\n",
    "                    mesh_large.append(out)\n",
    "\n",
    "            mesh_large_df = pd.DataFrame(mesh_large)\n",
    "            new_col_names = ['Preferred Label','Class ID','Semantic Types','Synonyms','Parents','CUI','AQL','TERMUI']\n",
    "            mesh_large_df.columns = new_col_names\n",
    "\n",
    "            ## Add STY Labels\n",
    "            mesh_large_df_sty = pd.merge(mesh_large_df, sty, on='Semantic Types', how='inner').reset_index(drop=True)\n",
    "            #Add rsid coulmn con merge\n",
    "            mesh_large_df_sty = mesh_large_df_sty.rename(columns={'Preferred Label_y':'Semantic Types Label','Preferred Label_x':'Preferred Label'})\n",
    "\n",
    "            mesh_large_df_sty = mesh_large_df_sty[['Preferred Label', 'Semantic Types Label', 'Class ID', 'Semantic Types', 'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']]\n",
    "            mesh_large_df_sty.to_csv('ref-mesh/MESH_STY.csv')\n",
    "            print('MESH_STY created')\n",
    "        else:\n",
    "            print('MESH.csv not available')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESH_STY_LITVAR1 imported from csv\n",
      "\n",
      " => For Reference MeSH list definition skip to Section 3\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------\n",
    "#### Create MESH-STY-LITVAR subset from MESH-STY.csv\n",
    "if os.path.exists('ref-mesh/MESH_STY_LITVAR1.csv'):\n",
    "    mesh_litvar_sty = pd.read_csv('ref-mesh/MESH_STY_LITVAR1.csv',index_col=0)\n",
    "    print('MESH_STY_LITVAR1 imported from csv')\n",
    "else:\n",
    "    grpm_mesh = pd.read_csv('ref-mesh/grpm_db_mesh.csv', index_col=0)\n",
    "    mask = mesh_large_df_sty['Preferred Label'].isin(grpm_mesh.mesh)\n",
    "    mesh_litvar_sty = mesh_large_df_sty[mask]\n",
    "    mesh_litvar_sty.to_csv('ref-mesh/MESH_STY_LITVAR1.csv')\n",
    "    print('MESH_STY_LITVAR1 created')\n",
    "\n",
    "print('\\n => For Reference MeSH list definition skip to Section 3')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:47:10.772083700Z",
     "start_time": "2025-02-21T10:47:10.403528100Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:47:14.330928Z",
     "start_time": "2025-02-21T10:47:14.143881200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESH_STY 348733 mesh\n",
      "MESH_STY 126 semantic types\n",
      "MESH_STY_LITVAR1 21705 mesh\n",
      "MESH_STY_LITVAR1 125 semantic types\n"
     ]
    },
    {
     "data": {
      "text/plain": "                  Preferred Label   Semantic Types Label  \\\n1       Electronic Health Records   Intellectual Product   \n2                   Consent Forms   Intellectual Product   \n3          Genealogy and Heraldry   Intellectual Product   \n4                    Publications   Intellectual Product   \n5         Pharmaceutical Services   Intellectual Product   \n...                           ...                    ...   \n516150          Animals, Congenic             Vertebrate   \n516151                Vertebrates             Vertebrate   \n516152                     Humans                  Human   \n516153                 Human Body                  Human   \n516155      Carbohydrate Sequence  Carbohydrate Sequence   \n\n                                                 Class ID  mesh_id  \\\n1       http://purl.bioontology.org/ontology/MESH/D057286  D057286   \n2       http://purl.bioontology.org/ontology/MESH/D032962  D032962   \n3       http://purl.bioontology.org/ontology/MESH/D005789  D005789   \n4       http://purl.bioontology.org/ontology/MESH/D011642  D011642   \n5       http://purl.bioontology.org/ontology/MESH/D010593  D010593   \n...                                                   ...      ...   \n516150  http://purl.bioontology.org/ontology/MESH/D020296  D020296   \n516151  http://purl.bioontology.org/ontology/MESH/D014714  D014714   \n516152  http://purl.bioontology.org/ontology/MESH/D006801  D006801   \n516153  http://purl.bioontology.org/ontology/MESH/D018594  D018594   \n516155  http://purl.bioontology.org/ontology/MESH/D002240  D002240   \n\n       Semantic Types  \n1                T170  \n2                T170  \n3                T170  \n4                T170  \n5                T170  \n...               ...  \n516150           T010  \n516151           T010  \n516152           T016  \n516153           T016  \n516155           T088  \n\n[31741 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Preferred Label</th>\n      <th>Semantic Types Label</th>\n      <th>Class ID</th>\n      <th>mesh_id</th>\n      <th>Semantic Types</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Electronic Health Records</td>\n      <td>Intellectual Product</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D057286</td>\n      <td>D057286</td>\n      <td>T170</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consent Forms</td>\n      <td>Intellectual Product</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D032962</td>\n      <td>D032962</td>\n      <td>T170</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Genealogy and Heraldry</td>\n      <td>Intellectual Product</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D005789</td>\n      <td>D005789</td>\n      <td>T170</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Publications</td>\n      <td>Intellectual Product</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D011642</td>\n      <td>D011642</td>\n      <td>T170</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pharmaceutical Services</td>\n      <td>Intellectual Product</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D010593</td>\n      <td>D010593</td>\n      <td>T170</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516150</th>\n      <td>Animals, Congenic</td>\n      <td>Vertebrate</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D020296</td>\n      <td>D020296</td>\n      <td>T010</td>\n    </tr>\n    <tr>\n      <th>516151</th>\n      <td>Vertebrates</td>\n      <td>Vertebrate</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D014714</td>\n      <td>D014714</td>\n      <td>T010</td>\n    </tr>\n    <tr>\n      <th>516152</th>\n      <td>Humans</td>\n      <td>Human</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D006801</td>\n      <td>D006801</td>\n      <td>T016</td>\n    </tr>\n    <tr>\n      <th>516153</th>\n      <td>Human Body</td>\n      <td>Human</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D018594</td>\n      <td>D018594</td>\n      <td>T016</td>\n    </tr>\n    <tr>\n      <th>516155</th>\n      <td>Carbohydrate Sequence</td>\n      <td>Carbohydrate Sequence</td>\n      <td>http://purl.bioontology.org/ontology/MESH/D002240</td>\n      <td>D002240</td>\n      <td>T088</td>\n    </tr>\n  </tbody>\n</table>\n<p>31741 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MeSH stats\n",
    "if 'mesh_large_df_sty' in globals():\n",
    "    print('MESH_STY', mesh_large_df_sty['Preferred Label'].nunique(), 'mesh')\n",
    "    print('MESH_STY', mesh_large_df_sty['Semantic Types Label'].nunique(), 'semantic types')\n",
    "\n",
    "print('MESH_STY_LITVAR1', mesh_litvar_sty['Preferred Label'].nunique(), 'mesh')\n",
    "print('MESH_STY_LITVAR1', mesh_litvar_sty['Semantic Types Label'].nunique(), 'semantic types\\n')\n",
    "\n",
    "mesh_litvar_sty[['Preferred Label', 'Semantic Types Label', 'Class ID', 'mesh_id','Semantic Types']]#.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ADD mesh_id col\n",
    "if 'mesh_id' not in mesh_litvar_sty.columns:\n",
    "    mesh_litvar_sty['mesh_id'] = mesh_litvar_sty['Class ID'].str.replace('http://purl.bioontology.org/ontology/MESH/', '')\n",
    "    new_order = ['Preferred Label', 'Semantic Types Label', 'Class ID', 'mesh_id', 'Semantic Types',\n",
    "                 'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']\n",
    "    mesh_litvar_sty = mesh_litvar_sty[new_order]\n",
    "    mesh_litvar_sty.to_csv('ref-mesh/MESH_STY_LITVAR1.csv')\n",
    "else:\n",
    "    print('mesh_id aready added to csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Mesh-STY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'mesh_large_df_sty' in globals():\n",
    "    # groupby and bar module\n",
    "    mesh_large_df_sty_less = mesh_large_df_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "    ### groupby.describe analysis by rsid--------------------\n",
    "    mesh_large_df_sty_less_count = mesh_large_df_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_large_df_sty_less_count.columns = mesh_large_df_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_large_df_sty_less_count.columns = new_column_names\n",
    "\n",
    "    mesh_large_df_sty_less_count_sort = mesh_large_df_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "    print('MESH_STY')\n",
    "    print('Semantic Type count:',len(mesh_large_df_sty_less_count_sort),'\\n\\nMeSH count for Sty:')\n",
    "    display(mesh_large_df_sty_less_count_sort[['mesh-count','Semantic Types Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph Barh\n",
    "num = 20\n",
    "x = mesh_large_df_sty_less_count_sort['Semantic Types Label'].iloc[:num]\n",
    "y = mesh_large_df_sty_less_count_sort['mesh-count'].iloc[:num]\n",
    "plt.figure(figsize=(4, len(x) *0.25))\n",
    "plt.title('Global Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "#plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "plt.ylabel('Semantic Types')\n",
    "plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "# use log scale\n",
    "plt.gca().set_xscale('log')\n",
    "#plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'Global \"Mesh-Semantic Type\" Zipf s law'\n",
    "x = mesh_large_df_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "y = mesh_large_df_sty_less_count_sort['mesh-count'].sort_values().iloc[:]\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title('Global \"Mesh-Semantic Type\" Zipf s law', loc='center',pad=10)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.ylabel('mesh number')\n",
    "plt.xlabel('semantic type rank', position=(0.5, 1.08))\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Density Rugged Plot\n",
    "data = mesh_large_df_sty_less_count_sort['mesh-count'].iloc[:]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(np.array(data), bw_method=0.5)\n",
    "sns.rugplot(np.array(data), color='r')\n",
    "\n",
    "#plt.yscale('log')\n",
    "plt.title('Mesh abundance for Semantic Type')\n",
    "#plt.yscale('log')\n",
    "print('Mesh abundance for Semantic Type:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze MESH-STY-LITVAR (subset of MESH-STY.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('mesh_litvar_sty mesh:',mesh_litvar_sty['Preferred Label'].nunique())\n",
    "\n",
    "memory = mesh_litvar_sty.memory_usage().sum()\n",
    "print(f'The memory_usage of mesh_litvar_sty is {memory/ (1024 * 1024):.2f} MB.')\n",
    "\n",
    "file_size = os.path.getsize('ref-mesh/MESH_STY_LITVAR1.csv')\n",
    "print(f'The size of MESH_STY_LITVAR1.csv is {file_size/ (1024 * 1024):.2f} MB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort MeSH-Sty-Litvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupby and bar module\n",
    "mesh_litvar_sty_less = mesh_litvar_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "### groupby.describe analysis by rsid--------------------\n",
    "mesh_litvar_sty_less_count = mesh_litvar_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "mesh_litvar_sty_less_count.columns = mesh_litvar_sty_less_count.columns.to_flat_index()\n",
    "new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "mesh_litvar_sty_less_count.columns = new_column_names\n",
    "\n",
    "mesh_litvar_sty_less_count_sort = mesh_litvar_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "print('MESH_STY_LITVAR1')\n",
    "mesh_litvar_sty_less_count_sort[['mesh-count','Semantic Types Label']]#.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graph Barh\n",
    "num = 20\n",
    "x = mesh_litvar_sty_less_count_sort['Semantic Types Label'].iloc[:num]\n",
    "y = mesh_litvar_sty_less_count_sort['mesh-count'].iloc[:num]\n",
    "plt.figure(figsize=(4, len(x)*0.25))\n",
    "plt.title('Litvar1 Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "plt.barh(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "plt.ylabel('Semantic Types')\n",
    "plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_label_position('top')\n",
    "# use log scale\n",
    "plt.gca().set_xscale('log')\n",
    "#plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'Global \"Mesh-Semantic Type\" Zipf s law'\n",
    "x = mesh_litvar_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "y = mesh_litvar_sty_less_count_sort['mesh-count'].sort_values().iloc[:]\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title('LitVar \"Mesh-Semantic Type\" Zipf s law', loc='center',pad=10)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.ylabel('mesh number')\n",
    "plt.xlabel('semantic type rank', position=(0.5, 1.08))\n",
    "\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VISUALIZE DIFFERENCES\n",
    "\n",
    "#reload sorted df\n",
    "if 'mesh_large_df_sty_less_count_sort' in locals() and isinstance(mesh_large_df_sty_less_count_sort, pd.DataFrame):\n",
    "    pass\n",
    "else:\n",
    "    mesh_large_df_sty_less = mesh_large_df_sty[['Preferred Label','Semantic Types Label']]\n",
    "    mesh_large_df_sty_less_count = mesh_large_df_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_large_df_sty_less_count.columns = mesh_large_df_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_large_df_sty_less_count.columns = new_column_names\n",
    "    mesh_large_df_sty_less_count_sort = mesh_large_df_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "if 'mesh_litvar_sty_less_count_sort' in locals() and isinstance(mesh_litvar_sty_less_count_sort, pd.DataFrame):\n",
    "    pass\n",
    "else:\n",
    "    mesh_litvar_sty_less = mesh_litvar_sty[['Preferred Label','Semantic Types Label']]\n",
    "    mesh_litvar_sty_less_count = mesh_litvar_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    mesh_litvar_sty_less_count.columns = mesh_litvar_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    mesh_litvar_sty_less_count.columns = new_column_names\n",
    "    mesh_litvar_sty_less_count_sort = mesh_litvar_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "\n",
    "# create two sample dataframes\n",
    "df1 = mesh_large_df_sty_less_count_sort[['mesh-count', 'Semantic Types Label']]\n",
    "df2 = mesh_litvar_sty_less_count_sort[['mesh-count', 'Semantic Types Label']]\n",
    "\n",
    "# add a 'source' column to each dataframe\n",
    "df1['source'] = 'global'\n",
    "df2['source'] = 'litvar'\n",
    "\n",
    "# combine the dataframes\n",
    "combined_df = pd.merge(df1, df2, on=['Semantic Types Label'], how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# sort the dataframe by column 'A'\n",
    "#combined_df = combined_df.sort_values('mesh-count')\n",
    "\n",
    "# reset the index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# display the combined dataframe\n",
    "combined_df = combined_df[-(combined_df['Semantic Types Label'] == 'Drug Delivery Device')]\n",
    "combined_df\n",
    "mesh_large_df_sty_less['Preferred Label'].nunique()\n",
    "combined_df = combined_df.sort_values(by='mesh-count_df2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# define a formatting function that generates a proportional bar\n",
    "def format_bar(value):\n",
    "    max_value = combined_df['mesh-count_df1'].max().max()  # get the maximum value in the dataframe\n",
    "    bar_width = int(value / max_value * 100)  # calculate the width of the bar as a percentage\n",
    "    return f'<div style=\"background-color: blue; width: {bar_width}%\">{value}</div>'\n",
    "\n",
    "def format_bar_2(value):\n",
    "    max_value = combined_df['mesh-count_df2'].max().max()  # get the maximum value in the dataframe\n",
    "    bar_width = int(value / max_value * 100)  # calculate the width of the bar as a percentage\n",
    "    return f'<div style=\"background-color: blue; width: {bar_width}%\">{value}</div>'\n",
    "\n",
    "# apply the formatting function to the dataframe\n",
    "df_formatted = combined_df.style.format({'mesh-count_df1': format_bar, 'mesh-count_df2': format_bar_2})\n",
    "\n",
    "# save the formatted dataframe to an HTML file\n",
    "with open('formatted_dataframe.html', 'w') as f:\n",
    "    f.write(df_formatted.render())\n",
    "\n",
    "# display the formatted dataframe\n",
    "df_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reference MeSH build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build coherent and comprehensive lists of mesh term related to different topics\n",
    "- nutrigentics\n",
    "- neurodegenerative diseases\n",
    "- skin diseases\n",
    "- infective diseases\n",
    "- reproductive physiology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available ref-MeSH lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:47:46.975615500Z",
     "start_time": "2025-02-21T10:47:46.901226600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available reference mesh lists:\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ng_aller',\n 'ng_cvd',\n 'ng_dmt2_ms',\n 'ng_eat_taste',\n 'ng_intol',\n 'ng_nutri',\n 'ng_ob_bmi',\n 'ng_oxi_stress',\n 'ng_vitam',\n 'ng_xeno']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check available refs:\n",
    "folder_path = \"ref-mesh\"  # Replace with the actual folder path\n",
    "#---------------------------\n",
    "\n",
    "# Create a file path pattern to match CSV files\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "\n",
    "# Use glob to get a list of file paths matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "csv_files_name = []\n",
    "# Print the list of CSV files\n",
    "for file in csv_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    csv_files_name.append(file_name)\n",
    "\n",
    "print('Available reference mesh lists:')\n",
    "csv_files_df = pd.Series(csv_files_name)\n",
    "ref_files_df = csv_files_df[csv_files_df.str.contains('ref_mesh_')].reset_index(drop=True)\n",
    "ref_files_df_small = csv_files_df[csv_files_df.str.contains('small_ref_mesh_ng_')].reset_index(drop=True)\n",
    "\n",
    "file_names = [x.split('mesh_')[1].split('.csv')[0] for x in ref_files_df]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ref-MeSH list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'mesh_litvar_sty' not in locals():\n",
    "    mesh_litvar_sty = pd.read_csv('ref-mesh/MESH_STY_LITVAR1.csv',index_col=0)\n",
    "    print('MESH_STY_LITVAR1 imported from csv')\n",
    "    \n",
    "mesh_litvar_sty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Ref-MeSH from Topic Term Lists\n",
    "For list of terms defined by the user: search in Preferred Labels and Synonyms corresponding mesh entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:48:54.139888Z",
     "start_time": "2025-02-21T10:48:54.075488700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    [Obesity, overweight and body weight control, ...\n1    [cardiovascular diseases, physiological proces...\n2    [Diabetes Melitus Type II and metabolic syndrome]\n3    [Vitamin metabolism and Vitamins recommended i...\n4               [eating behaviour and taste sensation]\n5                                  [food intolerances]\n6                                     [food allergies]\n7                      [diet-induced oxidative stress]\n8                          [metabolism of xenobiotics]\ndtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Biomedical topics\n",
    "nutritional_topic = [['diseases and disorders realted to nutrition and diet ', 'diet, food consuption, eating behaviour and nutrition']]\n",
    "infective_topic = [['infective agents, bacteria, virus and protozoan','infective diseases']]\n",
    "reproductive_topic = [['reproductive system physiology','reproductive system pathology', 'Assisted reproductive technology']]\n",
    "female_infertility_topic = [['female infertility, genetic imprinting and maternal effect']]\n",
    "\n",
    "nutritional_topics = [\n",
    "    ['Obesity, overweight and body weight control', 'compulsive eating behavior'],\n",
    "    ['cardiovascular diseases','physiological processes realted to cardiovascular diseases','lipid metabolism in the context of cardiovascular diseases'],\n",
    "    ['Diabetes Melitus Type II and metabolic syndrome'],\n",
    "    ['Vitamin metabolism and Vitamins recommended intake levels','Micronutrients metabolism and Micronutrient recommended intake levels', 'disease related to vitamins and micronutrients deficiency'],\n",
    "    ['eating behaviour and taste sensation'],\n",
    "    ['food intolerances'],\n",
    "    ['food allergies'],\n",
    "    ['diet-induced oxidative stress'],\n",
    "    ['metabolism of xenobiotics'],\n",
    "]\n",
    "chosen_topic = nutritional_topics\n",
    "pd.Series(chosen_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLTK MeSH Pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# download nltk requirements\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exampl', 'sentenc', 'demonstr', 'stem', 'stopword', 'remov', 'lower', 'oper', 'punctuat', 'remov']\n"
     ]
    }
   ],
   "source": [
    "# Updating libraries from NLTK and adding string for punctuation removal\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function for text preprocessing: tokenization, stopping, stemming, lowering, and punctuation removal\n",
    "def process_text(text, replace=False):\n",
    "    # Removing punctuation\n",
    "    if replace:\n",
    "        text = text.replace('-',' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lowercasing all tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "## Clean ref-mesh from biases term >>>>>>>>>>>>>>>\n",
    "def clean_mesh(get_rid_list, df, save_clean=False, path = 'ref-mesh/candidate_ref_mesh/'):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for get_rid in get_rid_list:\n",
    "        mask = df['Preferred Label'].str.contains(get_rid)\n",
    "        df = df[-mask]\n",
    "\n",
    "    if save_clean:\n",
    "        df.to_csv(path+'ref_mesh_'+topic_label+'.csv')\n",
    "    return df\n",
    "\n",
    "## Rip semantic-types from ref-mesh >>>>>>>>>>>>>>>\n",
    "def semantic_ripper(rip_list, topic_mesh_all, save=False, path = 'ref-mesh/candidate_ref_mesh/', tag=''):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    mask = topic_mesh_all['Semantic Types Label'].isin(rip_list)\n",
    "    topic_mesh_all_redux = topic_mesh_all[-mask]\n",
    "\n",
    "    if save:\n",
    "        topic_mesh_all_redux[['Preferred Label', 'Semantic Types Label', 'mesh_id']].to_csv(path+'ref_mesh_'+topic_label+tag+'.csv')\n",
    "    return topic_mesh_all_redux\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence demonstrating the stemming, stop-word removal, lowering operations, and punctuation removal!\"\n",
    "preprocessed_text = process_text(text)\n",
    "print(preprocessed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T13:39:44.065084700Z",
     "start_time": "2025-02-21T13:39:43.869950400Z"
    }
   },
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run Text processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31741/31741 [00:15<00:00, 2056.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     Preferred Label                     label_tokens  \\\n1          Electronic Health Records       [electron, health, record]   \n2                      Consent Forms                  [consent, form]   \n3             Genealogy and Heraldry             [genealog, heraldri]   \n4                       Publications                         [public]   \n5            Pharmaceutical Services             [pharmaceut, servic]   \n7              Programming Languages               [program, languag]   \n8   Mental Status and Dementia Tests  [mental, statu, dementia, test]   \n9                          Mythology                       [mytholog]   \n10               Models, Theoretical                 [model, theoret]   \n11             Models, Psychological               [model, psycholog]   \n\n       mesh_id  Semantic Types Label  \n1      D057286  Intellectual Product  \n2      D032962  Intellectual Product  \n3      D005789  Intellectual Product  \n4      D011642  Intellectual Product  \n5      D010593  Intellectual Product  \n7      D011381  Intellectual Product  \n8   D000073216  Intellectual Product  \n9      D009229  Intellectual Product  \n10     D008962  Intellectual Product  \n11     D008960  Intellectual Product  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Preferred Label</th>\n      <th>label_tokens</th>\n      <th>mesh_id</th>\n      <th>Semantic Types Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Electronic Health Records</td>\n      <td>[electron, health, record]</td>\n      <td>D057286</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consent Forms</td>\n      <td>[consent, form]</td>\n      <td>D032962</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Genealogy and Heraldry</td>\n      <td>[genealog, heraldri]</td>\n      <td>D005789</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Publications</td>\n      <td>[public]</td>\n      <td>D011642</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pharmaceutical Services</td>\n      <td>[pharmaceut, servic]</td>\n      <td>D010593</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Programming Languages</td>\n      <td>[program, languag]</td>\n      <td>D011381</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Mental Status and Dementia Tests</td>\n      <td>[mental, statu, dementia, test]</td>\n      <td>D000073216</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Mythology</td>\n      <td>[mytholog]</td>\n      <td>D009229</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Models, Theoretical</td>\n      <td>[model, theoret]</td>\n      <td>D008962</td>\n      <td>Intellectual Product</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Models, Psychological</td>\n      <td>[model, psycholog]</td>\n      <td>D008960</td>\n      <td>Intellectual Product</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#====================================================================================\n",
    "# Tokenize and process MeSH 'Preferred Labels' =======================================\n",
    "mother_df = mesh_litvar_sty\n",
    "replace = True\n",
    "\n",
    "# Use tqdm with apply to show a progress bar\n",
    "tqdm.pandas()\n",
    "mother_df['label_tokens'] = mother_df['Preferred Label'].progress_apply(process_text, replace = replace)\n",
    "mother_df[['Preferred Label','label_tokens','mesh_id','Semantic Types Label']].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T11:01:36.181235100Z",
     "start_time": "2025-02-21T11:01:20.723446Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import \"topic-terms\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "topic_terms_ng_eat_taste.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "folder_path = \"ref-mesh/topic_terms\"\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "csv_files = glob.glob(file_pattern)\n",
    "csv_files_name = []\n",
    "for file in csv_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    csv_files_name.append(file_name)\n",
    "\n",
    "#print('Available topic terms:')\n",
    "csv_files_df = pd.Series(csv_files_name)\n",
    "topic_terms_file = csv_files_df[csv_files_df.str.contains('topic_')]\n",
    "\n",
    "file_id = int(input('Available topic terms, select index:\\n'+str(pd.Series(topic_terms_file))))\n",
    "filename = topic_terms_file[file_id]\n",
    "topic_all_ser_df = pd.read_csv(folder_path+'/'+filename)\n",
    "\n",
    "topic_label = re.search('topic_terms_(.*).csv', filename).group(1)\n",
    "#-----------------------------------------------\n",
    "\n",
    "topic_all_ser = topic_all_ser_df.topic_terms.to_list()\n",
    "print(len(topic_all_ser))\n",
    "print(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T12:04:12.752328700Z",
     "start_time": "2025-02-21T12:04:08.032433500Z"
    }
   },
   "execution_count": 120
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve 'litvar_mesh' by 'topic-terms'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08.314038\n",
      "topic_mesh_ng_eat_taste created\n",
      "   topic terms: 186\n",
      "   mesh found 362\n",
      "   semantic groups 64\n",
      "ref_mesh_ng_eat_taste.csv already exists\n",
      "\n",
      "[Please validate manually MeSH List to remove ambiguity]\n"
     ]
    }
   ],
   "source": [
    "#get mesh\n",
    "topic_mesh_all = pd.DataFrame(columns=mother_df.columns) #Empty DataFrame, same structure\n",
    "\n",
    "time1 =datetime.now()\n",
    "for term in topic_all_ser:\n",
    "    term_tokens = process_text(term, replace = replace)\n",
    "    mask = mother_df['label_tokens'].apply(lambda x: all(term in x for term in term_tokens))\n",
    "    meshy = mother_df[mask]\n",
    "\n",
    "    topic_mesh_all = pd.concat([meshy,topic_mesh_all])\n",
    "time2 =datetime.now()\n",
    "print(time2-time1)\n",
    "\n",
    "#analyze semantics\n",
    "topic_mesh_all_group = topic_mesh_all.groupby('Semantic Types Label').describe()\n",
    "topic_mesh_all_group_sem = topic_mesh_all_group.index.to_list()\n",
    "\n",
    "new_name = 'topic_mesh_'+topic_label\n",
    "exec(f'{new_name} = topic_mesh_all')\n",
    "print(new_name, 'created')\n",
    "print('   topic terms:', len(topic_all_ser))\n",
    "print('   mesh found', topic_mesh_all['Preferred Label'].nunique())\n",
    "print('   semantic groups',topic_mesh_all['Semantic Types Label'].nunique())\n",
    "\n",
    "overwrite = False\n",
    "# GET ref mesh list\n",
    "if os.path.exists('ref-mesh/ref_mesh_'+topic_label+'.csv'):\n",
    "    print('ref_mesh_'+topic_label+'.csv already exists')\n",
    "    if overwrite:\n",
    "        globals()[new_name][['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh/ref_mesh_'+topic_label+'.csv')\n",
    "        print('overwritten')\n",
    "else:\n",
    "    globals()[new_name][['Preferred Label', 'Semantic Types Label']].to_csv('ref-mesh/ref_mesh_'+topic_label+'.csv')\n",
    "    print('\\nref_mesh_'+topic_label+'.csv saved')\n",
    "\n",
    "topic_mesh_all[['Preferred Label','mesh_id','Semantic Types Label']].drop_duplicates()\n",
    "\n",
    "print(\"\\n[Please validate manually MeSH List to remove ambiguity]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T12:04:29.210268200Z",
     "start_time": "2025-02-21T12:04:20.337732400Z"
    }
   },
   "execution_count": 121
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean and save 'candidate_ref_mesh'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Clean and save 'candidate_ref_mesh'\n",
    "folder_path_ = \"ref-mesh/candidate_ref_mesh\"\n",
    "if not os.path.exists(folder_path_):\n",
    "    os.makedirs(folder_path_)\n",
    "\n",
    "mesh_rid_list = ['oma','Apop','Cell','gen Ty','Cilia','Cytokinesis','DNA','Fluorescein','Intercellular Signaling','Leukem','Lysergic Acid','Loeys-Dietz','Mitochondrial','Peptide Hormones','Toll-Like','Tumor', 'Neoplasm', 'RNA','Gene','Diethyl', 'Nucleos', 'Motif', 'Domain', 'Virus', 'virus','Infant', 'Child', 'Adolescent', 'Elder', 'Maternal', 'Youth', 'Man', 'Woman', 'National', 'Neoplasm', 'Reproductive', 'Sexual', 'Genome', 'Animal', 'Doping', 'Social', 'Urban','Health C','Health E','Health F','Health I','Health P','Health S','Health T','ty Health','le Health','ic Health','Polymor','Genetic','Risk F','Proteins','Protein','DNA','RNA', 'Polymor','Genetic', 'Pharma', 'Cosme', 'Drug', 'Distribution']\n",
    "\n",
    "topic_mesh_all = clean_mesh(mesh_rid_list, topic_mesh_all, save_clean=False)\n",
    "\n",
    "semantic_rip_list = ['Cell', 'Receptor', 'Hormone', 'Tissue', 'Body Part, Organ, or Organ Component', 'Congenital Abnormality', 'Anatomical Abnormality', 'Indicator, Reagent, or Diagnostic Aid', 'Neoplastic Process', 'Mammal', 'Gene or Genome', 'Element, Ion, or Isotope']\n",
    "\n",
    "topic_mesh_all = semantic_ripper(semantic_rip_list, topic_mesh_all, save=True)\n",
    "topic_mesh_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ref-MeSH Refining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available ref-MeSH lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check avalable refs:\n",
    "folder_path = \"ref-mesh\"  # Replace with the actual folder path\n",
    "#---------------------------\n",
    "\n",
    "# Create a file path pattern to match CSV files\n",
    "file_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "\n",
    "# Use glob to get a list of file paths matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "csv_files_name = []\n",
    "# Print the list of CSV files\n",
    "for file in csv_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    csv_files_name.append(file_name)\n",
    "\n",
    "print('Available reference mesh lists:')\n",
    "csv_files_df = pd.Series(csv_files_name)\n",
    "ref_files_df = csv_files_df[csv_files_df.str.contains('ref_mesh_')].reset_index(drop=True)\n",
    "ref_files_df_small = csv_files_df[csv_files_df.str.contains('small_ref_mesh_ng_')].reset_index(drop=True)\n",
    "\n",
    "file_names = [x.split('mesh_')[1].split('.csv')[0] for x in ref_files_df]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_names = ['ref_'] * 10\n",
    "ref_names = [string + str(i) for i, string in enumerate(ref_names)]\n",
    "\n",
    "ref_list = []\n",
    "for name, ref, save  in zip(ref_names, ref_files_df, ref_files_df):\n",
    "    df = pd.read_csv('ref-mesh/'+ref, index_col=0).reset_index(drop=True)\n",
    "    df = df.drop('Semantic Types Label', axis=1).drop_duplicates()\n",
    "    df = df.sort_values(by='mesh_id',ascending=True)\n",
    "    globals()[name] = df\n",
    "    #df.to_csv('ref-mesh/small_'+save) # 'small' for representation purposes only\n",
    "    ref_list.append(df)\n",
    "\n",
    "ref_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Ref MeSH cleaning\n",
    "Human Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mesh Cleaner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get rid from bias generating MeSH and run again GRPM Module 3\n",
    "\n",
    "# put here manually selected mesh to purge\n",
    "xeno = ['Polymorphism, Genetic', 'DNA Methylation','Liver Neoplasms','Energy Metabolism','Metabolism, Inborn Errors','X-Ray Absorption Spectroscopy','Blood-Testis Barrier']\n",
    "oxi_stress = ['Food Deprivation','Food Analysis','Food, Formulated','Food Microbiology','Food Intolerance','Food Quality','Food Industry','Foods, Specialized','Food Chain','Foods, Specialized','Food Chain']\n",
    "intol = ['Depression','Osteoporosis','Immunotherapy','Anxiety','Anti-Inflammatory Agents, Non-Steroidal','Immunotherapy, Adoptive','Desensitization, Immunologic','Peanut Hypersensitivity','Milk Hypersensitivity', 'Egg Hypersensitivity','Immunotherapy, Active','Neurologic Manifestations' ,'Infectious Anemia Virus, Equine','Nut and Peanut Hypersensitivity' ,'Diarrhea Virus 1, Bovine Viral','Eye Movement Desensitization Reprocessing','Diarrhea Virus 2, Bovine Viral']\n",
    "eat_taste = ['Blood Glucose','Mouth Neoplasms','Glucose Transporter Type 1','Glucose Transporter Type 2','Auditory Perception','Citric Acid Cycle','Glucose Transporter Type 4','Loeys-Dietz Syndrome','United States Food and Drug Administration','Sodium-Glucose Transporter 2','Sodium-Glucose Transporter 2 Inhibitors','Glucose-6-Phosphate','Glucose Transporter Type 3','Pitch Perception','Depth Perception','Glucose-1-Phosphate Adenylyltransferase','Glucose Dehydrogenases','Glucosephosphates']\n",
    "cvd = ['DNA, Mitochondrial','Cell Differentiation','Protein Transport','Mitochondrial Proteins','Toll-Like Receptors','Genome, Mitochondrial','Genes, Mitochondrial','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Proton-Translocating ATPases','Mitochondrial Dynamics','Mitochondrial Membranes','Leukemia, Plasma Cell','Heart Neoplasms']\n",
    "dmt2_ms = [ 'MicroRNAs', 'Mitochondria', 'DNA, Mitochondrial', 'Mitochondrial Proteins', 'Pancreatic Neoplasms','Autophagy','Mitochondrial Diseases','Genome, Mitochondrial','Genes, Mitochondrial','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Precursor Protein Import Complex Proteins','Mitochondrial Dynamics','Mitochondrial Membranes','Mitochondrial Myopathies','RNA, Mitochondrial', 'Mitochondrial Encephalomyopathies','AIDS-Associated Nephropathy','Familial Primary Pulmonary Hypertension','Mitochondria, Heart','Autophagy-Related Protein-1 Homolog','Ocular Hypertension','Autophagy-Related Protein 7','Mitochondrial Trifunctional Protein','Mitochondrial Permeability Transition Pore','Mitochondrial Uncoupling Proteins','Mitochondrial ADP, ATP Translocases','Autophagy-Related Protein 8 Family','Mitochondrial Ribosomes','Mitochondrial Trifunctional Protein, alpha Subunit','Autophagy-Related Protein 12','Mitochondrial Turnover','Lysergic Acid Diethylamide','Mitochondrial Trifunctional Protein, beta Subunit','Chaperone-Mediated Autophagy','Mitochondrial Swelling','Mitochondrial Transmembrane Permeability-Driven Necrosis','Mitochondrial Size']\n",
    "ob_bmi = ['Infant','Child','Adolescent','Elder','Maternal','Youth','Man','Woman','National','Neoplasm''Epidemiolo','Reproductive','Sexual','Genome','Animal','Doping','Social','Urban''Health C','Health E','Health F','Health I','Health P','Health S','Health T','ty Health','le Health','ic Health','Polymor','Genetic','Risk F']\n",
    "\n",
    "# note: to list above stands for examples (ref mesh already cleaned in archive)\n",
    "\n",
    "#clean mesh ref\n",
    "get_rid_list = xeno\n",
    "\n",
    "#check mesh ref\n",
    "topic_label = 'ng_' + 'xeno'\n",
    "path_ = 'ref-mesh/candidate_ref_mesh/'\n",
    "dff = pd.read_csv(path_+'ref_mesh_'+topic_label+'.csv', index_col=0)\n",
    "\n",
    "dff_rid = clean_mesh(get_rid_list, dff, path = 'ref-mesh/candidate_ref_mesh/', save_clean =True)\n",
    "\n",
    "print(dff.describe(),'\\n-----------------------------------------------------------------')\n",
    "print(dff_rid.describe())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Semantic type ripper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Get rid from bias generating MeSH and run again GRPM Module 3\n",
    "\n",
    "semantic_rip_list = ['Cell', 'Receptor', 'Hormone', 'Tissue', 'Body Part, Organ, or Organ Component', 'Congenital Abnormality', 'Anatomical Abnormality', 'Indicator, Reagent, or Diagnostic Aid', 'Neoplastic Process', 'Mammal', 'Gene or Genome', 'Element, Ion, or Isotope']\n",
    "\n",
    "#check mesh ref\n",
    "topic_label = 'ng_' + 'ob_bmi'\n",
    "#path_ = 'ref-mesh/candidate_ref_mesh/'\n",
    "path_ = 'ref-mesh/'\n",
    "dff = pd.read_csv(path_+'ref_mesh_'+topic_label+'.csv', index_col=0)\n",
    "\n",
    "dff_rid = semantic_ripper(semantic_rip_list, dff, path = 'ref-mesh/', save=False)\n",
    "\n",
    "print(dff.describe(),'\\n-----------------------------------------------------------------')\n",
    "print(dff_rid.describe())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. create JSON sheet for ref-mesh lists\n",
    "https://jsoneditoronline.org/#left=local.lejobi&right=local.lejobi\n",
    "https://www.convertjson.com/json-to-html-table.htm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "folder_path = \"ref-mesh\"\n",
    "\n",
    "num_elements = len(ref_files_df)  # Number of elements you want in the list\n",
    "value_list = []\n",
    "for i in range(1, num_elements + 1):\n",
    "    element = \"file_pat\" + str(i)\n",
    "    value_list.append(element)\n",
    "\n",
    "jdata_list_n = ['jdata_'] * 10\n",
    "jdata_list_n = [string + str(i) for i, string in enumerate(jdata_list_n)]\n",
    "\n",
    "jdata_list = []\n",
    "for name in jdata_list_n:\n",
    "    empty =[]\n",
    "    globals()[name] = empty\n",
    "    jdata_list.append(empty)\n",
    "\n",
    "\n",
    "file_patterns = []\n",
    "choose_ref = ref_files_df # or ref_files_df_small\n",
    "for i in choose_ref:\n",
    "    file_patterns.append(os.path.join(folder_path, i))\n",
    "\n",
    "for i, j  in zip(range(len(ref_files_df)), jdata_list):\n",
    "\n",
    "    with open(file_patterns[i], 'r') as file1:\n",
    "        csv_reader = csv.DictReader(file1)\n",
    "        # for row in csv_reader:\n",
    "        #     j_data1.append(row)\n",
    "        for row in csv_reader:\n",
    "            modified_row = {key: value for key, value in row.items() if key != ''} #drop col\n",
    "            j.append(modified_row)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combined_data = {\n",
    "#    'data1': j_data1,\n",
    "#    'data2': j_data2 }\n",
    "\n",
    "combined_data = {}\n",
    "\n",
    "titles = [\n",
    "    \"Genaral Nutrition\",\n",
    "    \"Obesity, Weight Control and Compulsive Eating\",\n",
    "    \"Cardiovascular Health and Lipid Metabolism\",\n",
    "    \"Diabetes Mellitus Type II and Metabolic Syndrome\",\n",
    "    \"Vitamin and Micronutrients Metabolism and Deficiency-Related Diseases\",\n",
    "    \"Eating Behavior and Taste Sensation\",\n",
    "    \"Food Intolerances\",\n",
    "    \"Food Allergies\",\n",
    "    \"Diet-induced Oxidative Stress\",\n",
    "    \"Xenobiotics Metabolism\",\n",
    "]\n",
    "\n",
    "for tit, j_data in zip(titles, jdata_list):\n",
    "    key = tit\n",
    "    combined_data[key] = j_data\n",
    "\n",
    "#for i, j_data in enumerate(jdata_list, start=1):\n",
    "#    key = 'data' + str(i)\n",
    "#    combined_data[key] = j_data\n",
    "\n",
    "print(combined_data)\n",
    "#pd.DataFrame(combined_data)\n",
    "import pyperclip\n",
    "pyperclip.copy(str(combined_data))\n",
    "\n",
    "json_file = 'ref-mesh/nutrigenetis_referencemesh_small.json'\n",
    "with open(json_file, 'w') as outfile:\n",
    "    json.dump(combined_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. add mesh_id to MeSH list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add mesh id to every mesh list\n",
    "label = 'ng_nutri'\n",
    "dff = pd.read_csv('ref-mesh/ref_mesh_'+label+'.csv',index_col=0).reset_index(drop=True)#.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dff_id = pd.merge(dff, mesh_id_ref_df, left_on='Preferred Label', right_on='Preferred Label')\n",
    "dff_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dff_id.to_csv('ref-mesh/ref_mesh_'+label+'.csv')\n",
    "#grpm_db_merge_id = grpm_db_merge_id.drop('Preferred Label', axis=1)\n",
    "check = pd.read_csv('ref-mesh/ref_mesh_'+label+'.csv', index_col=0)\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chack = check.drop('mesh_id_y',axis=1)\n",
    "check = check.rename(columns={'mesh_id_y': 'mesh_id'})\n",
    "check.to_csv('ref-mesh/ref_mesh_'+label+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate Random MeSh sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random grpm mesh list from MESH-STY"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348733\n",
      "List saved: ref-mesh/random_lists/random_01.csv\n",
      "List saved: ref-mesh/random_lists/random_02.csv\n",
      "List saved: ref-mesh/random_lists/random_03.csv\n",
      "List saved: ref-mesh/random_lists/random_04.csv\n",
      "List saved: ref-mesh/random_lists/random_05.csv\n",
      "List saved: ref-mesh/random_lists/random_06.csv\n",
      "List saved: ref-mesh/random_lists/random_07.csv\n",
      "List saved: ref-mesh/random_lists/random_08.csv\n",
      "List saved: ref-mesh/random_lists/random_09.csv\n",
      "List saved: ref-mesh/random_lists/random_10.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate Random mesh list from complete df\n",
    "print(mesh_large_df_sty['Preferred Label'].nunique())\n",
    "\n",
    "# Drop duplicate entries\n",
    "mesh_large_df_sty_mesh = mesh_large_df_sty['Preferred Label'].drop_duplicates()\n",
    "\n",
    "# Define parameters\n",
    "random_path = 'ref-mesh/random_lists/'\n",
    "os.makedirs(random_path, exist_ok=True)\n",
    "size = 603\n",
    "random_states = [78954, 12245, 87498, 56798, 34565, 76523, 78968, 56845, 76624, 23845]\n",
    "\n",
    "# Loop through all random states\n",
    "for i, state in enumerate(random_states, start=1):\n",
    "    # Generate sample\n",
    "    sample = mesh_large_df_sty_mesh.sample(n=size, random_state=state)\n",
    "    # Save to CSV\n",
    "    sample.to_csv(f'{random_path}random_{i:02}.csv')\n",
    "    print(f'List saved: {random_path}random_{i:02}.csv')\n",
    "###"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:14:28.132510800Z",
     "start_time": "2025-02-21T15:14:27.611057400Z"
    }
   },
   "execution_count": 145
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random GRPM mesh list from MESH-STY-LITVAR\n",
    "Generate Random Mesh list from grpm df\n",
    "\n",
    "    grpm_db_mesh = pd.read_csv('ref-mesh/grpm_db_mesh.csv')\n",
    "    grpm_db_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:36:08.057067700Z",
     "start_time": "2025-02-21T15:36:07.814937700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grpm mesh imported from archive\n",
      "List saved: ref-mesh/random_lists/random_grpm_01.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_02.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_03.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_04.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_05.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_06.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_07.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_08.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_09.csv\n",
      "List saved: ref-mesh/random_lists/random_grpm_10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "if os.path.exists('ref-mesh/grpm_db_mesh.csv'):\n",
    "    grpm_db_mesh = pd.read_csv('ref-mesh/grpm_db_mesh.csv',index_col=0)\n",
    "    print('grpm mesh imported from archive')\n",
    "else:\n",
    "    grpmdb_table = pd.read_csv('grpm_dataset\\grpm_db_pcg\\grpm_table_output.csv')\n",
    "    grpm_db_mesh = pd.DataFrame(grpmdb_table.mesh.drop_duplicates())\n",
    "    type(grpm_db_mesh)\n",
    "    \n",
    "    \n",
    "# randomize 10 samples\n",
    "size = 450\n",
    "random_states = [78954, 12245, 87498, 56798, 34565, 76523, 78968, 56845, 76624, 23845]\n",
    "\n",
    "# Loop through all random states\n",
    "for i, state in enumerate(random_states, start=1):\n",
    "    # Generate sample\n",
    "    sample = grpm_db_mesh.mesh.sample(n=size, random_state=state)\n",
    "    # Save to CSV\n",
    "    sample.to_csv(f'{random_path}random_grpm_{i:02}.csv')\n",
    "    print(f'List saved: {random_path}random_grpm_{i:02}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#check fidelity\n",
    "mask= grpm_db_mesh.mesh.isin(sample_grpm_01)\n",
    "grpm_db_mesh[mask]\n",
    "#type(sample_grpm_10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### Refine index\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the random path directory\n",
    "random_path = 'ref-mesh/random_lists/'\n",
    "\n",
    "# Check if a specific CSV file exists\n",
    "samples = {}\n",
    "for i in range(10):\n",
    "    filename = f'random_grpm_{i:02}.csv'\n",
    "    # Read the CSV files into a dictionary of DataFrames\n",
    "    samples[i] = pd.read_csv(random_path + filename, index_col=0)\n",
    "\n",
    "# If save flag is True, save the samples back to CSV files\n",
    "save_rand = True\n",
    "if save_rand:\n",
    "    for i, df in samples.items():\n",
    "        filename = f'random_grpm_{i:02}.csv'\n",
    "        # Save each DataFrame to CSV\n",
    "        df.to_csv(random_path + filename)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:28:42.586281600Z",
     "start_time": "2025-02-21T15:28:42.337949700Z"
    }
   },
   "execution_count": 159
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crea 2 set da 5 random mesh senza ripetizioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Random grpm mesh list without rep from MESH-STY-LITVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:40:02.606526100Z",
     "start_time": "2025-02-21T15:40:02.540251700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "24                                        Essential Tremor\n736                                          Phenylalanine\n4758                                  Esophageal Neoplasms\n2190                               Mesothelioma, Malignant\n3446                      Receptors for Activated C Kinase\n                               ...                        \n31628    Mannosyl-Glycoprotein Endo-beta-N-Acetylglucos...\n650                     Keratoderma, Palmoplantar, Diffuse\n19305                                     Self-Help Groups\n1884                              Ion-Selective Electrodes\n767                               Choroid Plexus Neoplasms\nName: mesh, Length: 21755, dtype: object"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomize\n",
    "sample_grpm_full = grpm_db_mesh.mesh.sample(n=len(grpm_db_mesh), random_state = 782954)\n",
    "sample_grpm_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:40:03.684242Z",
     "start_time": "2025-02-21T15:40:03.630471100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 444 rows\n",
      "Chunk 2: 444 rows\n",
      "Chunk 3: 444 rows\n",
      "Chunk 4: 444 rows\n",
      "Chunk 5: 444 rows\n",
      "Chunk 6: 444 rows\n",
      "Chunk 7: 444 rows\n",
      "Chunk 8: 444 rows\n",
      "Chunk 9: 444 rows\n",
      "Chunk 10: 444 rows\n",
      "Chunk 11: 444 rows\n",
      "Chunk 12: 444 rows\n",
      "Chunk 13: 444 rows\n",
      "Chunk 14: 444 rows\n",
      "Chunk 15: 444 rows\n",
      "Chunk 16: 444 rows\n",
      "Chunk 17: 444 rows\n",
      "Chunk 18: 444 rows\n",
      "Chunk 19: 444 rows\n",
      "Chunk 20: 444 rows\n",
      "Chunk 21: 444 rows\n",
      "Chunk 22: 444 rows\n",
      "Chunk 23: 444 rows\n",
      "Chunk 24: 444 rows\n",
      "Chunk 25: 444 rows\n",
      "Chunk 26: 444 rows\n",
      "Chunk 27: 444 rows\n",
      "Chunk 28: 444 rows\n",
      "Chunk 29: 444 rows\n",
      "Chunk 30: 444 rows\n",
      "Chunk 31: 444 rows\n",
      "Chunk 32: 444 rows\n",
      "Chunk 33: 444 rows\n",
      "Chunk 34: 444 rows\n",
      "Chunk 35: 444 rows\n",
      "Chunk 36: 444 rows\n",
      "Chunk 37: 444 rows\n",
      "Chunk 38: 444 rows\n",
      "Chunk 39: 444 rows\n",
      "Chunk 40: 444 rows\n",
      "Chunk 41: 444 rows\n",
      "Chunk 42: 444 rows\n",
      "Chunk 43: 444 rows\n",
      "Chunk 44: 444 rows\n",
      "Chunk 45: 444 rows\n",
      "Chunk 46: 444 rows\n",
      "Chunk 47: 444 rows\n",
      "Chunk 48: 444 rows\n",
      "Chunk 49: 443 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\anaconda3\\envs\\GRPM_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# chunk size\n",
    "size = 450\n",
    "# Split the DataFrame into smaller DataFrames of chunk length 450\n",
    "chunks = np.array_split(sample_grpm_full, len(sample_grpm_full) // 450 + 1)\n",
    "\n",
    "# Print the number of chunks and the length of each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk {}: {} rows'.format(i+1, len(chunk)))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:41:17.874584500Z",
     "start_time": "2025-02-21T15:41:15.860210400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save each DataFrame chunk to a separate CSV file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(random_path + '/random_grpm_norep{}.csv'.format(i+1), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Random mesh list without repetetion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": "          random1  random2  random3  random4  random5  random6  random7  \\\nrandom1       444        0        0        0        0        0        0   \nrandom2         0      444        0        0        0        0        0   \nrandom3         0        0      444        0        0        0        0   \nrandom4         0        0        0      444        0        0        0   \nrandom5         0        0        0        0      444        0        0   \nrandom6         0        0        0        0        0      444        0   \nrandom7         0        0        0        0        0        0      444   \nrandom8         0        0        0        0        0        0        0   \nrandom9         0        0        0        0        0        0        0   \nrandom10        0        0        0        0        0        0        0   \n\n          random8  random9  random10  \nrandom1         0        0         0  \nrandom2         0        0         0  \nrandom3         0        0         0  \nrandom4         0        0         0  \nrandom5         0        0         0  \nrandom6         0        0         0  \nrandom7         0        0         0  \nrandom8       444        0         0  \nrandom9         0      444         0  \nrandom10        0        0       444  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>random1</th>\n      <th>random2</th>\n      <th>random3</th>\n      <th>random4</th>\n      <th>random5</th>\n      <th>random6</th>\n      <th>random7</th>\n      <th>random8</th>\n      <th>random9</th>\n      <th>random10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>random1</th>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random2</th>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>random10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>444</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COOCCURANCE MATRIX MODULE------------\n",
    "\n",
    "# Define the common path and file pattern\n",
    "random_path = 'ref-mesh/random_lists/'\n",
    "\n",
    "file_prefix = 'random_grpm_norep'\n",
    "file_extension = '.csv'\n",
    "\n",
    "# Use list comprehension to read CSV files and extract lists\n",
    "lists = [pd.read_csv(f'{random_path}/{file_prefix}{i}.csv').mesh.to_list() for i in range(1, 11)]\n",
    "\n",
    "# Get the number of lists\n",
    "num_lists = len(lists)\n",
    "\n",
    "# Initialize a 2D list of zeros with dimensions equal to the number of lists\n",
    "cooccur_matrix = [[0] * num_lists for i in range(num_lists)]\n",
    "type(cooccur_matrix)\n",
    "print(len(set(lists[1]) & set(lists[1])))\n",
    "print(len(set(lists[1]) & set(lists[3])))\n",
    "\n",
    "# Loop over all pairs of lists and count the number of co-occurring elements\n",
    "for i in range(num_lists):\n",
    "    for j in range(num_lists):\n",
    "        if i == j:\n",
    "            cooccur_matrix[i][j] = len(lists[i])\n",
    "        else:\n",
    "            cooccur_matrix[i][j] = len(set(lists[i]) & set(lists[j]))\n",
    "\n",
    "# Convert the 2D list to a Pandas DataFrame\n",
    "cooccur_df = pd.DataFrame(cooccur_matrix, columns=['random{}'.format(i+1) for i in range(num_lists)], index=['random{}'.format(i+1) for i in range(num_lists)])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "cooccur_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-21T15:53:59.933379100Z",
     "start_time": "2025-02-21T15:53:59.805057Z"
    }
   },
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Print the resulting matrix with row and column headers\n",
    "print(', '.join([''] + ['list{}'.format(i+1) for i in range(num_lists)]))\n",
    "\n",
    "for i in range(num_lists):\n",
    "    row = ['list{}'.format(i+1)]\n",
    "    row.extend(cooccur_matrix[i])\n",
    "    print(', '.join(str(x) for x in row))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add mesh_id to GRPM dataset (not required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"note: in GRPM Dataset screening using 'Preferred Label' results faster then numerical 'mesh_id'\")\n",
    "\n",
    "if simple_bool(\"note: in GRPM Dataset screening using 'Preferred Label' results faster then numerical 'mesh_id'\\n add mesh_id to grpm_ds and replace 'Preferred Label' anyway?\") and os.path.exists('grpm_dataset/grpm_db_pcg/grpm_table_output.csv'):\n",
    "    grpm_b_df = pd.read_parquet('grpm_dataset/grpm_dataset.parquet')\n",
    "\n",
    "    if not 'mesh_id' in grpm_b_df.columns:\n",
    "        # add mesh_id to grpm db\n",
    "        mesh_id_ref_df = mesh_litvar_sty[['Preferred Label','mesh_id']]\n",
    "        grpm_db_merge_id = pd.merge(grpm_b_df, mesh_id_ref_df, left_on='mesh', right_on='Preferred Label')\n",
    "\n",
    "        # replace 'Preferred Label' with 'mesh_id'\n",
    "        grpm_db_merge_id = grpm_db_merge_id.drop('Preferred Label', axis=1)\n",
    "        grpm_db_merge_id.columns\n",
    "        new_order = ['gene', 'rsid', 'pmid', 'mesh_id']\n",
    "        grpm_db_merge_id = grpm_db_merge_id[new_order].drop_duplicates()\n",
    "        grpm_db_merge_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if simple_bool('save it?'):\n",
    "    grpm_db_merge_id.to_parquet('grpm_dataset/grpm_datset_id.parquet')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if simple_bool('check?'):\n",
    "    check = pd.read_parquet('grpm_dataset/grpm_datset_id.parquet')\n",
    "    check"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Reference MeSH list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "if os.path.exists('ref-mesh/ref_mesh_nest.csv'):\n",
    "    ref = pd.read_csv('ref-mesh/ref_mesh_nest.csv', sep=\";\")\n",
    "    print('nested ref mesh', ref['Class ID'].nunique())\n",
    "\n",
    "    ref['Semantic Types'] = ref['Semantic Types'].apply(ast.literal_eval)\n",
    "    #otherwise= dfg['col1'] = df['col1'].str.strip('[]').str.split(', ').apply(lambda x: [int(i) for i in x])\n",
    "    sty = pd.read_csv('ref-mesh/MeshSTY-code.csv',sep=';')\n",
    "    sty = sty.rename(columns={'ID':'Semantic Types'})\n",
    "    print(ref)\n",
    "else:\n",
    "    print('ref_mesh_nest.csv not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'ref' in globals():\n",
    "    ref_large = []\n",
    "    for i in range(len(ref)):\n",
    "        for sem in ref['Semantic Types'][i]: #dfrspost = mother table\n",
    "            out = ref['Preferred Label'][i],ref['Class ID'][i],sem,ref['Synonyms'][i],ref['Parents'][i],ref['CUI'][i],ref['AQL'][i],ref['TERMUI'][i]\n",
    "            ref_large.append(out)\n",
    "\n",
    "    ref_large_df = pd.DataFrame(ref_large)\n",
    "    new_col_names = ['Preferred Label','Class ID','Semantic Types','Synonyms','Parents','CUI','AQL','TERMUI']\n",
    "    ref_large_df.columns = new_col_names\n",
    "    display(ref_large_df)\n",
    "    #ref_large_df.to_csv('ref-mesh/ref_mesh_large.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh semantics analyze on Ref list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## set df on ref mesh sty large\n",
    "\n",
    "if os.path.exists('ref-mesh/ref_mesh_sty_large.csv'):\n",
    "    ref_sty = pd.read_csv('ref-mesh/ref_mesh_sty_large.csv', index_col=0)\n",
    "else:\n",
    "    ref_sty = pd.merge(ref_large_df, sty, on='Semantic Types', how='inner').reset_index(drop=True)\n",
    "    #Add rsid coulmn con merge\n",
    "    #rspmidmesh_merge = pd.merge(pmidmesh, rsidpmid, on= 'pmid', how='inner').drop_duplicates().reindex(columns=['pmid', 'rsid', 'mesh'])\n",
    "    ref_sty = ref_sty.rename(columns={'Preferred Label_y':'Semantic Types Label','Preferred Label_x':'Preferred Label'})\n",
    "    #ref_sty = ref_sty.drop('Column1',axis=1)\n",
    "    #ref_sty.to_csv('ref-mesh/ref_mesh_sty_large.csv')\n",
    "\n",
    "ref_sty = ref_sty[['Preferred Label', 'Semantic Types Label', 'Class ID', 'Semantic Types', 'Synonyms', 'Parents', 'CUI', 'AQL', 'TERMUI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ref_sty['Preferred Label'].nunique(), 'mesh total')\n",
    "ref_sty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'ref_sty' in globals():\n",
    "    #modulo groupby and bar\n",
    "    ref_sty_less = ref_sty[['Preferred Label','Semantic Types Label']]\n",
    "\n",
    "    ### groupby.describe analysis by rsid--------------------\n",
    "    ref_sty_less_count = ref_sty_less.groupby('Semantic Types Label').describe().reset_index()\n",
    "    ref_sty_less_count.columns = ref_sty_less_count.columns.to_flat_index()\n",
    "    new_column_names = ['Semantic Types Label', 'mesh-count', 'mesh-unique','mesh-top','mesh-freq']\n",
    "    ref_sty_less_count.columns = new_column_names\n",
    "    ref_sty_less_count_sort = ref_sty_less_count.sort_values(by='mesh-count',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    x = ref_sty_less_count_sort['Semantic Types Label'].iloc[:]\n",
    "    y = ref_sty_less_count_sort['mesh-count'].iloc[:]\n",
    "    plt.figure(figsize=(4, len(ref_sty_less_count_sort)*0.25))\n",
    "    plt.title('Reference Mesh- Semantic Type enrichment', loc='center',pad=10)\n",
    "\n",
    "    plt.barh(x,y)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tick_params(axis='x', which='both', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "    #plt.xlabel('pmid count', position=(0.5, 1.08))\n",
    "    plt.ylabel('Semantic Types')\n",
    "    plt.xlabel('mesh', position=(0.5, 1.08))\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    # use log scale\n",
    "    plt.gca().set_xscale('log')\n",
    "    #plt.savefig('Reference Mesh- Semantic Type enrichment.png',dpi=300, bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_sty_less_count_sort#['Semantic Types Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# analizing abundance\n",
    "#count = [12, 21]\n",
    "#remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count'].isin(count)]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count']> 56]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "ref_sty[ref_sty['Semantic Types Label'].isin(remove_value)]['Semantic Types Label'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STY Ripper form complete ref mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Rip form complete ref mesh\n",
    "save = False\n",
    "rip_list = ['Cell', 'Receptor', 'Hormone', 'Tissue', 'Body Part, Organ, or Organ Component', 'Congenital Abnormality', 'Anatomical Abnormality', 'Indicator, Reagent, or Diagnostic Aid']\n",
    "\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['Semantic Types Label'].isin(rip_list)]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "remove_value = ref_sty_less_count_sort[ref_sty_less_count_sort['mesh-count']> 56]['Semantic Types Label'].reset_index(drop=True).to_list()\n",
    "\n",
    "#new_ref_sty = ref_sty[ref_sty['Semantic Types Label'].isin(remove_value)]\n",
    "mask = ref_sty['Semantic Types Label'].isin(rip_list)\n",
    "new_ref_sty = ref_sty[-mask]\n",
    "\n",
    "if save == True:\n",
    "    new_ref_sty.to_csv('ref-mesh/new_ref_mesh_large_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ref_sty['Preferred Label'].drop_duplicates().to_csv('ref-mesh/new_ref_mesh_corrected.csv')\n",
    "new_ref_sty['Preferred Label'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Mesh df break through:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for all sons from a parent ID (trial)\n",
    "    goal: merge all child mesh together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Search for all brothers from a parent ID:\n",
    "var4 = 'D000066888' #'Diet, Food, and Nutrition'\n",
    "var5 = 'D044623' #'Nutrition Therapy'\n",
    "var6 = 'D014808' #Nutritional and Metabolic Diseases\n",
    "var7 = 'D002318'#Cardiovascular Diseases\n",
    "var8 = 'D004066'#Digestive System Diseases\n",
    "var9 = 'D004700'#Endocrine System Diseases\n",
    "var10 = 'D006967'#Hypersensitivity\n",
    "\n",
    "sub = df[df['Parents'].str.contains(var4).fillna(False)]\n",
    "# use .dropna(inplace=True)   OR   df.fillna(0, inplace=True)\n",
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfgb = pd.DataFrame(columns=df.columns) #Empty DataFrame, same structure\n",
    "#dfgg = pd.DataFrame()\n",
    "dfgg = sub\n",
    "# I MUST search in 'Parents' for the top category ID.\n",
    "\n",
    "#Parents Class ID list:\n",
    "kids = sub['Class ID'].tolist() #search for nested childs\n",
    "\n",
    "# Multiple search:\n",
    "for i in kids:\n",
    "    child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "    dfgg = dfgg.append([child])\n",
    "    dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "    kids = child['Class ID'].tolist()\n",
    "    for i in kids:\n",
    "        child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "        dfgg = dfgg.append([child])\n",
    "        #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "        kids = child['Class ID'].tolist()\n",
    "        for i in kids:\n",
    "            child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "            dfgg = dfgg.append([child])\n",
    "            #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "            kids = child['Class ID'].tolist()\n",
    "            for i in kids:\n",
    "                child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "                dfgg = dfgg.append([child])\n",
    "                #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "                kids = child['Class ID'].tolist()\n",
    "                for i in kids:\n",
    "                    child = df[df['Parents'].str.contains(i).fillna(False)]\n",
    "                    dfgg = dfgg.append([child])\n",
    "                    #dfgb = pd.concat([child, dfgb.loc[:]])\n",
    "                    kids = child['Class ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfgg.loc[:].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chat_mesh_nutri#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create new empty dataframe\n",
    "dfg = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "ds1 = df[df['Class ID'].str.contains(var3)]\n",
    "ds2 = df[df['Class ID'].str.contains(var4)]\n",
    "#dfg.append(arr, ignore_index=True)\n",
    "#dfg2 = dfg.append(ds1)\n",
    "dfg2 = pd.concat([ds2, dfg2.loc[:]])\n",
    "dfg2\n",
    "#USE CONCAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd= df[['Preferred Label','MeSH Frequency']]\n",
    "ddf= dd.dropna(subset='Preferred Label')\n",
    "print(ddf.shape)\n",
    "print(dd.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(dd['Preferred Label'], dd['MeSH Frequency'])\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[\"Preferred Label\"].__contains__(\"human\")\n",
    "df2=df[df[\"Preferred Label\"].str.contains('Polymorphism')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2=df[df[\"Preferred Label\"].str.contains('Diabetes')]\n",
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2=df[df[\"Preferred Label\"].str.contains('Fabry')]\n",
    "df3= df2[['Preferred Label','MeSH Frequency']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# links\n",
    " #https://www.nlm.nih.gov/databases/download/data_distrib_main.html\n",
    " #https://www.nlm.nih.gov/databases/download/mesh.html\n",
    " #https://id.nlm.nih.gov/mesh/swagger/ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
